{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fax Tutorial\n",
    "============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello friend.\n",
    "Welcome to the basic tutorial on how to simulate waveforms with the lastest fax version in strax.\n",
    "Here we'll just demonstrate the basic functionality. For more indepth analysis stuff, checkout the straxen tutorials for more detailed thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import strax\n",
    "import straxen\n",
    "import wfsim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from multihist import Histdd, Hist1d\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting everything up\n",
    "================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to define the right context. The thing which differs now is where to get the plugin to provide raw records. By default this is the DAQ Reader. Now we do not want this so we register wfsim.RawRecordsFromFax. I think it is self explanatory where this plugin tells strax to get raw records from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://raw.githubusercontent.com/XENONnT/strax_auxiliary_files/master/fax_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = strax.Context(\n",
    "    register=wfsim.RawRecordsFromFax,\n",
    "    config=dict(detector=\"XENONnT\",\n",
    "                fax_config=url_base + 'fax_config_nt.json'),\n",
    "    **straxen.contexts.common_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the XENONnT configuration you'll need to make a small modification to strax. Strax has a number of tpc channels hardcoded and you'll need to overwrite that with the correct nT number of channels\n",
    "To do so you'll need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "straxen.plugins.pulse_processing.n_tpc = 494\n",
    "straxen.plugins.PeakPositions.n_top_pmts = 253"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally you'll also want to get strax to use the right pmt gains and a nT neural net for position reconstruction\n",
    "For this do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.set_config(dict(to_pe_file=url_base + 'to_pe_nt.npy',\n",
    "                   nn_architecture = url_base + 'mlp_model.json',\n",
    "                   nn_weights = url_base +  'mlp_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define a run id. What you give it doesn't really matter, since strax will look for data and make new if it doesn't find anything. And this is what you want.\n",
    "Strax will use the run id to get the electron lifetime and pmt gains from a database, and returns placeholders if the run doesn't exist. (Currently the electron lifetime doesn't return a placeholder, this should be fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strax has a build in timeout which we need to modify. When simulating stuff we'll probably take more time then is allowed by the timeout so we need to increase it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strax.Mailbox.DEFAULT_TIMEOUT=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining instructions\n",
    "==============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last detail before we can start. To give fax instructions you now have two possibilities. Either read in a MC output file and let a super basic nestpy convert it to instructions, or have them be random generated.\n",
    "First I'll show how to read from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/dali/lgrandi/pgaemers/fax_files/Xenon1T_WholeLXe_Pb212_00008_g4mc_G4.root'\n",
    "st.set_config(dict(fax_file= file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative is to let fax make some random things for you. This will call the function strax_interface.rand_instructions in case you want to change it up a little bit.\n",
    "We need to tell fax 3 parameters. nchunk tells strax over how many files to smear out the data. Currently it is highy advised to set this to 1 to avoid crashes. event_rate determains how many events per second to make,  so this will determaine, approximatly, the spacing between events. Finally chunk_size defines the length of a chunk in seconds.\n",
    "The total number of events generated is the product of all three numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.set_config(dict(fax_file=None))\n",
    "st.set_config(dict(nchunk=2, event_rate = 1, chunk_size = 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you also get do is define your own instruction generating function and overwrite the default one.\n",
    "You can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_awesome_custom_instruction(c):\n",
    "    n = c['nevents'] = c['event_rate'] * c['chunk_size'] * c['nchunk']\n",
    "    c['total_time'] = c['chunk_size'] * c['nchunk']\n",
    "\n",
    "    instructions = np.zeros(2 * n, dtype=wfsim.strax_interface.instruction_dtype)\n",
    "    uniform_times = c['total_time'] * (np.arange(n) + 0.5) / n\n",
    "    instructions['time'] = np.repeat(uniform_times, 2) * int(1e9)\n",
    "    instructions['event_number'] = np.digitize(instructions['time'],\n",
    "         1e9 * np.arange(c['nchunk']) * c['chunk_size']) - 1\n",
    "    instructions['type'] = np.tile([1, 2], n)\n",
    "    instructions['recoil'] = ['er' for i in range(n * 2)]\n",
    "\n",
    "    r = np.sqrt(np.random.uniform(0, 2500, n))\n",
    "    t = np.random.uniform(-np.pi, np.pi, n)\n",
    "    instructions['x'] = np.repeat(r * np.cos(t), 2)\n",
    "    instructions['y'] = np.repeat(r * np.sin(t), 2)\n",
    "    instructions['z'] = np.repeat(np.random.uniform(-100, 0, n), 2)\n",
    "\n",
    "    nphotons = np.random.uniform(2000, 2050, n)\n",
    "    nelectrons = 10 ** (np.random.uniform(1, 4, n))\n",
    "    instructions['amp'] = np.vstack([nphotons, nelectrons]).T.flatten().astype(int)\n",
    "\n",
    "    return instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instruction function gets called with a config, and from this it will take the event rate, chunk size and number of chunks to figure out how many events you. wanted. to have.\n",
    "The things you are (probably?) most interested in playing. with are the position and signal sizes. \n",
    "\n",
    "Finally you evaluate the following line, which will. tell the simulator to execute your. custom function rather then the predefined one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfsim.strax_interface.rand_instruction = super_awesome_custom_instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What actually happens?\n",
    "===\n",
    "\n",
    "\n",
    "What happens behind the scenes is that the instructions are first grouped together in chunks. Then we loop over the instructions and the full chunk is returned before starting with the next one.\n",
    "\n",
    "We use a S1 and S2 class to calculate the arrival times of the photons and the channels which have been hit. Then we'll hand them over to the Pulse class to calculate the currents in the channels. Finally the currents go to a RawData class where we fake the digitizer response.\n",
    "\n",
    "S1\n",
    "==\n",
    "For S1s we start with calculating the light yield based on the position of the interaction, and draw the number of photons seen from a poisson distribution.\n",
    "\n",
    "Second we calculate the arrival times of the photons. This is based on the scintiallation of the xenon atoms. It is dependend on the recombination time, the singlet and triplet fractions.\n",
    "\n",
    "Finally the channels are calcuated. Based on the pattern map we use a interpolation map to get a probability distribution for channels to be hit for a S1 signal based on the position of the interaction, and then we draw from this distribution for every photon.\n",
    "\n",
    "S2\n",
    "===\n",
    "S2s are slightly more complicated. First we need to drift the electrons up, and while doing so we'll lose some of them.\n",
    "To get the photon timings, we first need to get the arrival times of the electrons at the gas interface based on a diffusion model. Then we can calculate the photon timings based on a luminesience model for every individual electron. And for the channels we do the same trick with the interpolating map.\n",
    "\n",
    "\n",
    "Pulse\n",
    "===\n",
    "When we have our lists of channels and timing we can generate actual pulses. First we add a pmt transition time. Then we loop over all channels, calculate the double pe emission probabilities, and add a current in the pmt channel based on the arrival time. This is all stored in a big dictionary. Afterwards this is passed to our fake digitizers which then returns you with your very own pretty data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting down to bussiness\n",
    "---\n",
    "\n",
    "Now we have acces to all the normal strax data types, and another one called 'truth' which holds the simulation instructions. Calling it follows the normal strax convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove any previously simulated data, if such exists\n",
    "# !rm -r strax_data\n",
    "\n",
    "records = st.get_array(run_id,'records')\n",
    "peaks = st.get_array(run_id, ['peaks','peak_classification'])\n",
    "data = st.get_df(run_id, 'event_info')\n",
    "\n",
    "truth = st.get_df(run_id, 'truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to make pretty plots and see if what we makes actually makes any sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = records[records['channel']==10]\n",
    "ns = np.arange(len(records['data'][0]))\n",
    "plt.plot(ns, r['data'][:10].mean(axis=0), linestyle='steps-mid')\n",
    "plt.fill_between(\n",
    "    ns,\n",
    "    np.percentile(r['data'], 25, axis=0),\n",
    "    np.percentile(r['data'], 75, axis=0),\n",
    "    step='mid', alpha=0.3, linewidth=0)\n",
    "plt.xlabel(\"Sample in record\")\n",
    "plt.ylabel(\"Amplitude (ADCc)\")\n",
    "plt.title('Average  amplitude in a channel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(peaks[peaks['type']==1]['data'][:10].T)\n",
    "plt.xlabel(\"Sample in record\")\n",
    "plt.ylabel(\"Amplitude (PE/sample)\")\n",
    "plt.title(\"Some S1's\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(peaks[peaks['type']==2]['data'][:10].T)\n",
    "plt.xlabel(\"Sample in record\")\n",
    "plt.ylabel(\"Amplitude (PE/sample)\")\n",
    "plt.title(\"Some S2's\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Since the instructions are generated randomly I do not know what your results look like, so I make some assumptions):\n",
    "\n",
    "Wauw, they really look amazing, right? For further analysis, we'd like to look at if the created peaks more or less look like the instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of found events is {len(data)}, while the number of events in the instruction was {len(truth)/2} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = st.get_df(run_id, 'truth')\n",
    "data = st.get_df(run_id, 'event_info')\n",
    "\n",
    "truth = truth[(truth['type'] == 1) & (truth['n_photon'] > 0)]\n",
    "truth.sort_values(by='t_first_photon', inplace=True)\n",
    "timing_grid = truth['t_first_photon']\n",
    "\n",
    "truth['merge_index'] = np.digitize(truth['time'], timing_grid)\n",
    "data['merge_index'] = np.digitize(data['time'], timing_grid)\n",
    "\n",
    "truth.drop('event_number', axis=1, inplace=True)\n",
    "data = data.merge(truth, how='outer', left_on='merge_index', right_on='merge_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "mh = Histdd(data.s1_area, data.n_photon,\n",
    "            bins=[np.logspace(0, 2.2, 101), np.logspace(0, 2.2, 101)])\n",
    "plt.pcolormesh(mh.bin_edges[0], mh.bin_edges[1], mh.histogram.T, norm=LogNorm())\n",
    "plt.xscale('log'); plt.yscale('log')\n",
    "plt.xlabel('S1 (reconstructed)')\n",
    "plt.ylabel('S1 (truth)')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "mh = Histdd(data.n_photon, (data.s1_area-data.n_photon)/data.n_photon, \n",
    "            bins=[np.logspace(0, 2.2, 101), np.linspace(-0.5, 0.5, 101)])\n",
    "plt.pcolormesh(mh.bin_edges[0], mh.bin_edges[1], mh.histogram.T, norm=LogNorm())\n",
    "plt.xscale('log')\n",
    "plt.xlabel('S1 (reconstructed)')\n",
    "plt.ylabel('Bias')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = st.get_df(run_id, 'truth')\n",
    "data = st.get_df(run_id, 'event_info')\n",
    "\n",
    "truth = truth[(truth['type'] == 2) & (truth['n_photon'] > 0)]\n",
    "truth.sort_values(by='t_first_photon', inplace=True)\n",
    "timing_grid = truth['t_first_photon']\n",
    "\n",
    "truth['merge_index'] = np.digitize(truth['time'], timing_grid)\n",
    "data['merge_index'] = np.digitize(data['time'], timing_grid)\n",
    "truth.drop('event_number', axis=1, inplace=True)\n",
    "data = data.merge(truth, how='outer', left_on='merge_index', right_on='merge_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "mh = Histdd(data.s2_area, data.n_photon,\n",
    "            bins=[np.logspace(2, 4.5, 121), np.logspace(2, 4.5, 121)])\n",
    "plt.pcolormesh(mh.bin_edges[0], mh.bin_edges[1], mh.histogram.T, norm=LogNorm())\n",
    "plt.xscale('log'); plt.yscale('log')\n",
    "plt.xlabel('S2 (reconstructed)')\n",
    "plt.ylabel('S2 (truth)')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "mh = Histdd(data.n_photon, (data.s2_area-data.n_photon)/data.n_photon, \n",
    "            bins=[np.logspace(2, 4.5, 121), np.linspace(-0.5, 0.5, 121)])\n",
    "plt.pcolormesh(mh.bin_edges[0], mh.bin_edges[1], mh.histogram.T, norm=LogNorm())\n",
    "plt.xscale('log')\n",
    "plt.xlabel('S2 (reconstructed)')\n",
    "plt.ylabel('Bias')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
